# DeepSeek-V2의 연구 목표 및 주요 결과

**DeepSeek-V2**는 **2360억(236B)** 규모의 **Mixture-of-Experts (MoE)** 기반 대형 언어 모델로, **경제적인 훈련 비용**과 **효율적인 추론**을 동시에 달성하는 것을 목표로 개발되었습니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=%3E%20Abstract%3AWe%20present%20DeepSeek,stronger%20performance%2C%20and%20meanwhile%20saves)). 이 모델은 거대한 전체 파라미터를 가지면서도 **토큰당 활성화되는 부분은 약 210억(21B)**에 불과하여, 계산 자원을 절약하면서도 높은 성능을 유지합니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=%3E%20Abstract%3AWe%20present%20DeepSeek,stronger%20performance%2C%20and%20meanwhile%20saves)). 주요 연구 목표는 **대규모 파라미터의 효율적 사용**과 **긴 문맥 길이(128K 토큰)** 지원이며, 이를 통해 소수의 자원으로도 최첨단 수준의 성능을 내는 것입니다.

- **혁신적 아키텍처 도입**: DeepSeek-V2는 **Multi-head Latent Attention (MLA)**와 **DeepSeekMoE**라는 새로운 구조를 도입하여 **메모리 병목**과 **훈련 비용** 문제를 해결했습니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=236B%20total%20parameters%2C%20of%20which,and%20boosts%20the%20maximum)). MLA는 **Key-Value 캐시**를 획기적으로 압축하여 긴 문맥에서도 추론 속도를 높이고, DeepSeekMoE는 **희소 활성화**를 통해 거대 모델을 낮은 비용으로 훈련할 수 있게 합니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=236B%20total%20parameters%2C%20of%20which,and%20boosts%20the%20maximum)).

- **주요 성능 향상**: 이전 세대 모델(DeepSeek 67B)과 비교해 **성능이 크게 향상**되었으며, **훈련 비용은 42.5% 절감**되고, **KV 캐시 메모리는 93.3% 감소**, **생성 속도(throughput)는 5.76배** 향상되는 뛰어난 결과를 보였습니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=into%20a%20latent%20vector%2C%20while,and%20boosts%20the%20maximum)). 이러한 개선으로 DeepSeek-V2는 **동일 토큰당 21B 파라미터만으로**도 **대규모 밀집(dense) 67B 모델을 능가하는 성능**을 달성했습니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=into%20a%20latent%20vector%2C%20while,tier%20performance)).

- **상위권 성능 달성**: 8.1조 토큰에 이르는 방대한 고품질 말뭉치로 사전학습되고, 지도 미세조정(SFT)과 강화학습(RL)을 거친 DeepSeek-V2는, 활성화 파라미터가 적음에도 불구하고 **개방형 모델 중 최고 수준의 성능**을 기록했습니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=generation%20throughput%20to%205,source%20models)). 실제 평가 결과, DeepSeek-V2는 **자신보다 2~3.5배 큰 모델들과 비슷하거나 그 이상의 성능**을 주요 벤치마크에서 보여주었는데, 예를 들어 **21B 활성 파라미터로 170~240B 규모 모델과 대등한 성능을 달성**하였습니다 ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=When%20combined%20with%20MLA%2C%20this,a%20drastically%20lower%20compute%20budget)). 이런 성과를 내면서도 **요구되는 연산 자원과 비용은 크게 낮추었다**는 점에서 의미가 큽니다 ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=When%20combined%20with%20MLA%2C%20this,a%20drastically%20lower%20compute%20budget)).

# Multi-Head Latent Attention (MLA)의 구조 및 작동 방식

**MLA (Multi-Head Latent Attention)**는 **Transformer의 어텐션 메커니즘**에서 **KV 캐시**로 인한 **메모리 병목**을 해결하기 위해 DeepSeek-V2에서 고안된 방법입니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=236B%20total%20parameters%2C%20of%20which,and%20boosts%20the%20maximum)). 일반적인 멀티헤드 어텐션에서는 각 토큰마다 모든 헤드에 대한 **Key와 Value 벡터**를 전체 모델 차원 크기로 저장해야 하므로, 문맥 길이가 길어질수록 메모리 사용량이 선형적으로 증가합니다. MLA는 **저차원 잠재 벡터(latent vector)**로 Key와 Value를 **공동 압축**하여, **KV 캐시의 크기를 대폭 줄이면서도 성능을 유지**하도록 설계되었습니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=236B%20total%20parameters%2C%20of%20which,and%20boosts%20the%20maximum)). 

핵심 아이디어는 **Key-Value의 **저랭크(low-rank) 공동 압축**입니다 ([A Visual Walkthrough of DeepSeek’s Multi-Head Latent Attention (MLA) 🧟‍♂️ | Towards AI](https://towardsai.net/p/artificial-intelligence/a-visual-walkthrough-of-deepseeks-multi-head-latent-attention-mla-%EF%B8%8F#:~:text=The%20core%20idea%20behind%20MLA,to%20reduce%20the%20KV%20cache)). 이를 위해 각 토큰의 은닉표현 $h_t$에 대해 다음과 같은 과정을 거칩니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=In%20standard%20attention%2C%20each%20token%27s,them%20into%20smaller%20%E2%80%9Clatent%E2%80%9D%20vectors)) ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=1,fly%20from%20the%20latent%20vector)):

1. **다운프로젝션(압축)** – 우선 학습된 가중치 행렬 $W^{DKV}$를 사용해 토큰의 숨겨진 상태 $h_t$ (차원 $d_{\text{model}}$)를 **더 작은 잠재 차원** $d_c$로 투영합니다. 이렇게 얻어진 벡터 $\mathbf{c}_t^{KV}$가 해당 토큰의 **압축된 KV 잠재 벡터**입니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=In%20standard%20attention%2C%20each%20token%27s,them%20into%20smaller%20%E2%80%9Clatent%E2%80%9D%20vectors)). 예를 들어 모델 차원이 4096인 경우, $W^{DKV}$를 통해 이를 1024 차원짜리 잠재 벡터로 줄이는 식입니다 ([A Visual Walkthrough of DeepSeek’s Multi-Head Latent Attention (MLA) 🧟‍♂️ | Towards AI](https://towardsai.net/p/artificial-intelligence/a-visual-walkthrough-of-deepseeks-multi-head-latent-attention-mla-%EF%B8%8F#:~:text=However%2C%20researchers%20working%20on%20DeepSeek,store%20the%20KV%20cache%20efficiently)) ([A Visual Walkthrough of DeepSeek’s Multi-Head Latent Attention (MLA) 🧟‍♂️ | Towards AI](https://towardsai.net/p/artificial-intelligence/a-visual-walkthrough-of-deepseeks-multi-head-latent-attention-mla-%EF%B8%8F#:~:text=,with%20model%20dimension%20of%204096)). 이 벡터 $\mathbf{c}_t^{KV}$는 **해당 토큰의 Key와 Value 정보를 함축**하고 있으며, 추론 중 **KV 캐시로 저장되는 것은 $\mathbf{c}_t^{KV}$ 하나면 충분**합니다 ([A Visual Walkthrough of DeepSeek’s Multi-Head Latent Attention (MLA) 🧟‍♂️ | Towards AI](https://towardsai.net/p/artificial-intelligence/a-visual-walkthrough-of-deepseeks-multi-head-latent-attention-mla-%EF%B8%8F#:~:text=When%20we%E2%80%99re%20making%20predictions%20with,also%20speeds%20up%20the%20process)). (기존에는 토큰당 4096차원의 Key와 4096차원의 Value, 총 두 벡터를 저장해야 했던 것을, 이제 **1024차원의 하나의 벡터만 저장**하면 되는 셈입니다. 이처럼 잠재 벡터로 대체함으로써 KV 캐시 메모리가 획기적으로 감소합니다.)

2. **업프로젝션(복원)** – 어텐션 스코어 계산 시에는, 저장해두었던 $\mathbf{c}_t^{KV}$로부터 다시 **원래 차원의 Key와 Value**를 복원합니다. 이를 위해 $\mathbf{c}_t^{KV}$에 별도의 가중치 행렬 $W^{UK}$와 $W^{UV}$를 적용하여 **Key** 벡터 $\mathbf{k}_t^{C}$와 **Value** 벡터 $\mathbf{v}_t^{C}$를 얻습니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=In%20standard%20attention%2C%20each%20token%27s,them%20into%20smaller%20%E2%80%9Clatent%E2%80%9D%20vectors)). 즉, 하나의 잠재 벡터를 두 가지 행렬로 투영함으로써 해당 토큰의 Key와 Value를 **동시에 근사 복원**하는 것입니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=In%20standard%20attention%2C%20each%20token%27s,them%20into%20smaller%20%E2%80%9Clatent%E2%80%9D%20vectors)). 이렇게 생성된 Key, Value를 이용해 일반적인 멀티헤드 어텐션과 동일하게 Query와의 내적 연산을 수행하여 어텐션 출력을 계산합니다.

3. **KV 캐시 저장 및 활용** – **추론 시에는 각 토큰에 대해 다운프로젝션된 잠재 벡터 $\mathbf{c}_t^{KV}$만 캐시에 저장**하고, 매 새로운 토큰을 처리할 때 필요한 이전 토큰들의 Key와 Value는 이 잠재 벡터들을 통해 **온더플라이(on-the-fly)** 재구성합니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=1,fly%20from%20the%20latent%20vector)). 이 방법으로 **KV 캐시의 메모리 사용을 매우 효율적으로 관리**할 수 있으며, GPU 메모리 대역폭 병목을 완화하여 **추론 지연을 줄이는 효과**를 얻습니다 ([A Visual Walkthrough of DeepSeek’s Multi-Head Latent Attention (MLA) 🧟‍♂️ | Towards AI](https://towardsai.net/p/artificial-intelligence/a-visual-walkthrough-of-deepseeks-multi-head-latent-attention-mla-%EF%B8%8F#:~:text=DeepSeek%20came%20up%20with%20a,speed%20up%20the%20inference%20process)) ([A Visual Walkthrough of DeepSeek’s Multi-Head Latent Attention (MLA) 🧟‍♂️ | Towards AI](https://towardsai.net/p/artificial-intelligence/a-visual-walkthrough-of-deepseeks-multi-head-latent-attention-mla-%EF%B8%8F#:~:text=The%20core%20idea%20behind%20MLA,to%20reduce%20the%20KV%20cache)).

**MLA의 효과**: 이러한 잠재 벡터 기법으로 DeepSeek-V2는 **128K 토큰에 달하는 긴 문맥을 처리하면서도 KV 캐시 메모리를 기존의 1/10 이하로 축소**할 수 있었습니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=including%20Multi,and%20boosts%20the%20maximum)). 실제로 KV 캐시 크기가 약 **93.3% 감소**하여, 메모리 자원 소모가 크게 줄었음에도 **멀티헤드 어텐션과 비슷한 성능을 유지**했음을 논문에서 보고하고 있습니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=into%20a%20latent%20vector%2C%20while,and%20boosts%20the%20maximum)) ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=%2A%20Memory%3A%2093.3,extended%20context%20windows%20by%2032x)). 요약하면, MLA는 **KV를 잠재공간에 압축-보관-복원**함으로써 **추론 메모리 병목을 해결**하고 **긴 문맥에서도 효율적인 어텐션 계산**을 가능하게 한 기술입니다.

# DeepSeekMoE의 구조 및 작동 방식

**DeepSeekMoE**는 DeepSeek-V2의 **Feed-Forward Network (FFN)** 부분에 적용된 **Mixture-of-Experts** 구조를 가리킵니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=236B%20total%20parameters%2C%20of%20which,and%20boosts%20the%20maximum)). 일반 Transformer의 FFN은 모든 토큰에 대해 동일한 파라미터로 연산되는 **밀집(dense)** 형태지만, MoE에서는 다수의 **전문가 FFN(Expert)**들이 존재하고, 각 토큰마다 그 중 일부만 선택되어 활성화됩니다. 이렇게 하면 **모델의 총 파라미터 수를 매우 크게 늘리면서도**(예: DeepSeek-V2의 총 파라미터 236B) **실제 각 토큰 처리에 사용하는 연산은 일부 전문가에 한정**되므로 계산 비용을 절감할 수 있습니다 ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=When%20combined%20with%20MLA%2C%20this,a%20drastically%20lower%20compute%20budget)). DeepSeekMoE는 이러한 MoE 개념을 발전시켜 **훈련 안정성**과 **하드웨어 효율**을 높인 고성능 MoE 아키텍처입니다 ([deepseek-ai/DeepSeek-V2 · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2#:~:text=,stronger%20models%20at%20lower%20costs)) ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=Next%2C%20they%20adopt%20a%20Mixture,forward%20blocks)).

구조적으로 DeepSeekMoE의 FFN 층은 **“공유 전문가”**와 **“라우팅 전문가”**로 구성됩니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=Next%2C%20they%20adopt%20a%20Mixture,forward%20blocks)):

- **공유 전문가 (Shared Experts)**: **모든 토큰에 공통으로 적용되는 FFN**입니다. 이들은 각 토큰의 입력을 항상 처리하여, **토큰 불문하고 유용한 보편적 패턴**을 모델링합니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=Next%2C%20they%20adopt%20a%20Mixture,forward%20blocks)). (예를 들어, 언어의 기초적인 문법 패턴이나 일반 상식 등의 처리는 공유 전문가가 맡을 수 있습니다.)

- **라우팅 전문가 (Routed Experts)**: **특정 토큰에만 선택적으로 적용되는 FFN**들입니다. 이들은 **전문화된 하위 문제**를 다루도록 학습되며, 어느 토큰에 어느 전문가를 할당할지는 **게이팅(gating) 네트워크**가 결정합니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=Next%2C%20they%20adopt%20a%20Mixture,forward%20blocks)). 게이팅은 각 토큰의 특성에 따라 어떤 전문가들이 가장 관련성이 높은지 판단하여 **상위 $K$개의 전문가를 선택**하고 해당 토큰의 FFN으로 사용합니다. DeepSeek-V2의 경우 한 토큰당 두 개 정도의 전문가를 활성화하는 **Top-2** 희소 활성화를 적용한 것으로 알려져 있습니다 (이를 통해 **토큰마다 전체 236B 중 약 21B 파라미터만 사용**되도록 설계된 것입니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=%3E%20Abstract%3AWe%20present%20DeepSeek,stronger%20performance%2C%20and%20meanwhile%20saves))). 

DeepSeekMoE에서 **게이팅 메커니즘의 개선**으로 도입된 개념 중 하나는 **“디바이스 제한 라우팅 (device-limited routing)”**입니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=They%20further%20limit%20cross,basic%20process%20is%20as%20follows)). 이는 **멀티 GPU 분산 환경에서 통신 비용을 줄이기 위한 전략**으로, 다음과 같이 동작합니다:

1. **관련 장치 선택** – 우선 각 토큰에 대해, **가장 적합한 전문가들이 위치한 상위 $M$개의 디바이스(GPU)**를 식별합니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=They%20further%20limit%20cross,basic%20process%20is%20as%20follows)). (예를 들어 32개의 전문가가 4대의 GPU에 나눠져 있다면, 특정 토큰에 대해 게이팅 네트워크가 **특정 1~2개의 GPU를 우선 선택**하는 식입니다.)

2. **국소 전문가 선택** – 선택된 $M$개의 디바이스 내에서 그 토큰과 **가장 높은 affinity를 보이는 상위 $K_r$개의 전문가**를 고릅니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=,experts%20to%20process%20the%20token)). 이때 $K_r$는 토큰당 활성화할 전문가 수로, DeepSeek-V2에서는 보통 2개 정도로 설정되었습니다. 이렇게 하면 토큰당 최종적으로 선택되는 전문가는 **전체 전문가 풀 중 극히 일부**로 제한됩니다.

3. **전문가 할당 및 처리** – 최종 선택된 1~2개의 전문가에게 해당 토큰의 FFN 연산을 할당하여 처리합니다. 나머지 전문가들은 이 토큰에 대해서는 계산을 수행하지 않으므로 **연산 자원과 메모리를 절약**할 수 있습니다. 앞선 예에서, 각 토큰은 32명의 전문가 중 오직 2명에게만 전달되어 처리되고, 이 2명이 속한 GPU와만 통신하면 되므로 **장치 간 통신량**도 크게 감소합니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=They%20further%20limit%20cross,basic%20process%20is%20as%20follows)).

**예시**: 32개의 FFN 전문가가 4대의 GPU에 8개씩 분산되어 있다고 가정해보겠습니다. 전통적인 MoE에서는 한 토큰이 32개의 전문가 중 상위 2명을 고르면, 이 둘이 **다른 GPU에 있을 경우 모든 해당 GPU와 통신**해야 합니다. 그러나 DeepSeekMoE의 디바이스 제한 라우팅을 쓰면, 게이팅은 우선 **관련 전문가가 많은 상위 1개의 GPU**를 선택하고 그 GPU 내에서 2명의 전문가를 고릅니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=They%20further%20limit%20cross,basic%20process%20is%20as%20follows)). 이렇게 하면 **한 토큰을 처리할 때 오직 1개의 GPU상의 전문가들만 통신에 참여**하게 되어, 다수 GPU에 걸친 데이터 이동이 제거됩니다. 결과적으로, **하드웨어 상의 통신 병목을 줄여** 대용량 MoE 모델의 확장성을 높일 수 있습니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=Without%20device,introduces%20three%20balancing%20loss%20functions)).

DeepSeekMoE가 **안정적으로 학습**되기 위해서는 특정 전문가나 특정 장치에 **부하가 치우치는 현상**을 방지해야 합니다. 이를 위해 **세 가지 균형 손실 항목(balance loss)**이 도입되었습니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=DeepSeek%20team,introduces%20three%20balancing%20loss%20functions)):

- **전문가 균형 손실 ($L_{ExpBal}$)**: 일부 전문가만 선택되고 다른 전문가가 거의 선택되지 않는 **전문가 쏠림 현상**을 막기 위한 손실입니다. 토큰-전문가 affinity의 softmax 분포 등을 이용해 **토큰이 전문가들에게 고르게 분배되도록** 유도합니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=DeepSeekMoE%20introduces%20three%20balancing%20loss,functions)).

- **디바이스 균형 손실 ($L_{DevBal}$)**: 특정 디바이스(GPU)에만 토큰 처리가 몰리는 것을 방지하는 손실입니다. **장치 간 작업량이 균등**하게 유지되도록 가이드하여, 자원 활용을 고르게 합니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=%2A%20Expert,token%20routing%20to%20each%20device)).

- **통신 균형 손실 ($L_{CommBal}$)**: 각 디바이스로 **들어오고 나가는 토큰 라우팅의 균형**을 맞추는 손실입니다. 한 디바이스가 지나치게 많은 토큰을 받거나 보내는 것을 완화하여 **통신 병목을 방지**합니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=,token%20routing%20to%20each%20device)).

이러한 **게이팅 전략 개선과 균형화 기법** 덕분에 DeepSeek-V2는 **MoE 특유의 불안정성 없이** 훈련을 수행할 수 있었으며, 결과적으로 **파라미터 규모 대비 뛰어난 효율**을 얻을 수 있었습니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=Training%20%26%20Outcomes)). 요약하면, DeepSeekMoE는 **필요한 경우에만 일부 전문가를 사용하는 희소 구조**로 **연산량을 줄이고**, **스마트한 라우팅과 균형 기법**으로 **효율성과 안정성**을 모두 잡은 FFN 아키텍처입니다.

# DeepSeek-V2와 유사한 MoE 기반 모델들의 비교 및 성능 평가

대규모 언어 모델 분야에서 **Mixture-of-Experts (MoE)** 접근은 **Google** 등의 선도기업에서도 연구되어 왔습니다. 예를 들어, **구글의 Switch Transformer**는 2021년에 발표된 MoE 모델로 **총 1조개 이상의 파라미터**를 가지면서 토큰당 일부(예: 64개 중 1개) 전문가만 활성화하여 **모델 효율을 높인 대표 사례**입니다 ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=Mixtral9%2C%20Microsoft%27s%20Retentive%20Network10%2C%20and,efficient%20scaling%20without%20complex%20load)). 이후 **GLaM** 모델 등 다양한 MoE 기반 모델들이 나왔고, 심지어 **GPT-4 역시 MoE를 사용한 것으로 추정**될 만큼 이 접근은 **규모 확장의 유망한 방향**으로 인식되고 있습니다 ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=This%20matters%20because%20MoE%20architectures,without%20complex%20load%20balancing%20schemes)). 그러나 기존 MoE 모델들은 **부하 균형을 위한 보조 손실(auxiliary loss)** 설정 등이 복잡하고, 전문가 간 **통신 오버헤드 문제**로 인해 실용적인 구현이 어렵다는 단점이 있었습니다 ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=This%20matters%20because%20MoE%20architectures,without%20complex%20load%20balancing%20schemes)). DeepSeek-V2는 앞서 설명한 **MLA+MoE의 결합**을 통해 이러한 한계를 극복하고자 했습니다. 특히 **MLA로 메모리 병목을 제거**하고, **개선된 MoE 게이팅으로 복잡한 부하 균형 기법 없이도 효율적 스케일링**을 달성한 것이 독특한 점입니다 ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=This%20matters%20because%20MoE%20architectures,without%20complex%20load%20balancing%20schemes)). 이는 **기존 기법들을 효과적으로 조합하여 처음으로 실현**했다는 평가를 받고 있습니다 ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=I%20hope%20what%27s%20clear%20by,the%20first%20to%20combine%20them)) ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=,improvement%20during%20inference)).

DeepSeek-V2의 **독특한 특징**을 요약하면 다음과 같습니다:

- **초대용량 **희소 모델**임에도 **경제적인 훈련**: 236B의 거대 모델이지만 **토큰당 21B 파라미터**만 사용하도록 함으로써, 필요한 연산량과 메모리를 줄였습니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=%3E%20Abstract%3AWe%20present%20DeepSeek,stronger%20performance%2C%20and%20meanwhile%20saves)). 그 결과, **동일 성능 기준으로 기존 밀집 모델 대비 훨씬 적은 비용으로 훈련**이 가능했습니다 (논문에서는 DeepSeek 67B 대비 **42.5% 낮은 비용**으로 더 높은 성능을 달성했다고 보고합니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=into%20a%20latent%20vector%2C%20while,and%20boosts%20the%20maximum))). 한 분석에 따르면 DeepSeek-V2(및 후속 모델)는 불과 **\$560만 달러 수준의 훈련 비용**으로 GPT-4나 LLaMA3 등의 수십억 달러대 모델과 견줄 성능을 얻어 **효율성 면에서 혁신**을 보여주었습니다 ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=Except%20DeepSeek%20trounced%20them%20all,3)) ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=When%20combined%20with%20MLA%2C%20this,a%20drastically%20lower%20compute%20budget)).

- **MLA로 **긴 문맥** 지원 및 추론 최적화**: DeepSeek-V2는 MLA 덕분에 **128K 토큰의 긴 컨텍스트**를 처리하면서도 KV 캐시 메모리 사용량을 극도로 줄였습니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=236B%20total%20parameters%2C%20of%20which,and%20boosts%20the%20maximum)). 이는 기존 모델들이 4K~8K 정도의 문맥 길이에 머무는 것과 대비되며, 긴 문맥에서도 **속도 저하 없이** 동작하는 강점을 제공합니다 ([[2405.04434] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434#:~:text=including%20Multi,and%20boosts%20the%20maximum)). 예컨대 **Needle In A Haystack (NIAH)** 같은 긴 문맥 벤치마크 테스트에서 128K 문맥까지 원활히 동작하는 성능을 보였다고 보고되고 있습니다 ([deepseek-ai/DeepSeek-V2 · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2#:~:text=Image)).

- **개선된 MoE 아키텍처**: DeepSeekMoE는 앞서 언급한 **디바이스 제한 라우팅**과 **다중 균형 손실** 등을 통해 **기존 MoE의 통신 병목과 전문가 미사용 문제**를 완화했습니다 ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=They%20further%20limit%20cross,basic%20process%20is%20as%20follows)) ([The DeepSeek Series: A Technical Overview](https://martinfowler.com/articles/deepseek-papers.html#:~:text=DeepSeek%20team,introduces%20three%20balancing%20loss%20functions)). 이에 따라 **모델 활용률을 극대화**하면서도 **안정적으로 학습**되었고, 추가적인 복잡한 보조 기법 없이도 전문가들이 자연스럽게 고르게 활용되는 결과를 얻었습니다 ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=This%20matters%20because%20MoE%20architectures,without%20complex%20load%20balancing%20schemes)). 이러한 접근은 이전의 Switch Transformer나 GLaM 등에서 겪었던 **전문가 불균형 문제를 완화**한 점에서 차별화됩니다.

- **다분야에 걸친 우수한 성능**: DeepSeek-V2는 영어, 중국어, 코드, 수학 등의 **다양한 벤치마크에서 일관되게 상위권** 성능을 보였습니다 ([deepseek-ai/DeepSeek-V2 · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2#:~:text=MMLU%20English%2078,6)). 아래는 주요 벤치마크에서의 DeepSeek-V2 (236B MoE)와 다른 모델들의 **성능 비교**입니다 (괄호 안은 해당 모델의 파라미터 규모):

  - **영어 지식 테스트 (MMLU)**: DeepSeek-V2 **78.5점**으로, **LLaMA3 70B**(밀집, 70B)의 78.9점에 근접하며, **Mixtral 8×22B**(MoE, 총 176B) 모델의 77.6점을 약간 상회했습니다 ([deepseek-ai/DeepSeek-V2 · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2#:~:text=Benchmark%20Domain%20LLaMA3%2070B%20Mixtral,2)). 이는 DeepSeek-V2가 **자신보다 훨씬 큰 MoE 모델과 대등한 지식 응답 능력**을 보임을 뜻합니다.

  - **영어 추론/상식 (BBH)**: DeepSeek-V2는 **78.9점**으로, LLaMA3 70B의 81.0점에 근접했고 **Mixtral 176B와는 동일한 수준(78.9)**이었습니다 ([deepseek-ai/DeepSeek-V2 · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2#:~:text=MMLU%20English%2078,9)). 과거 DeepSeek-67B 밀집 모델의 68.7점과 비교하면 크게 향상된 수치로, **MoE로 용량을 늘린 효과**가 드러납니다.

  - **중국어 벤치마크 (C-Eval & CMMLU)**: DeepSeek-V2는 **중국어에서 특히 뛰어난 성능**을 나타냈습니다. 예를 들어 **C-Eval**에서 **81.7점**을 기록하여, LLaMA3 70B의 67.5점을 크게 앞질렀고 ([deepseek-ai/DeepSeek-V2 · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2#:~:text=MMLU%20English%2078,6)), **CMMLU**에서도 84.0점으로 LLaMA3의 69.3점을 압도했습니다 ([deepseek-ai/DeepSeek-V2 · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2#:~:text=C,8)). 이는 **다언어 학습 데이터**로 훈련된 DeepSeek-V2의 강점을 보여주며, 중국어 분야에서는 동급 최강 수준임을 의미합니다.

  - **코드 생성 (HumanEval)**: 코딩 능력 평가인 **HumanEval**에서 DeepSeek-V2는 **48.8%** 정답률을 보여 **LLaMA3 70B(48.2%)와 유사한 수준의 성능**을 냈습니다 ([deepseek-ai/DeepSeek-V2 · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2#:~:text=C,6)). 또 다른 코드 벤치마크인 MBPP에서도 66.6%로 LLaMA3 70B(68.6%)에 근접하였습니다 ([deepseek-ai/DeepSeek-V2 · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2#:~:text=HumanEval%20Code%2048,2)). 이는 **MoE 구조가 코드 생성 성능을 저해하지 않으며**, 대형 밀집 모델과 대등한 실력을 발휘함을 시사합니다.

  - **수학적 추론 (GSM8K, MATH)**: **GSM8K** 산술추론 벤치마크에서 DeepSeek-V2는 **79.2점**으로 LLaMA3 70B의 83.0점에 다소 못 미쳤지만, 이전 67B 모델의 63.4점에 비해 월등히 향상되었습니다 ([deepseek-ai/DeepSeek-V2 · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2#:~:text=HumanEval%20Code%2048,6)). 또 **MATH** 대수 문제 벤치마크에서는 43.6점으로 LLaMA3 70B(42.2점)와 **비슷하거나 약간 우세한 결과**를 보였습니다 ([deepseek-ai/DeepSeek-V2 · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2#:~:text=HumanEval%20Code%2048,6)). 이는 **MoE 모델임에도 고차원 수학 추론에서 경쟁력**을 유지함을 보여줍니다.

이처럼 DeepSeek-V2는 **주요 벤치마크 전반에서 70B~170B급 최신 모델들과 어깨를 나란히하거나 그 이상의 성능**을 시현했습니다. 특히 **중국어 및 다국어, 코드 분야에서 강점**을 보이고 있으며, 대화형으로 튜닝된 **DeepSeek-V2-Chat** 모델의 경우에는 **영어/중국어 오픈엔드 대화 평가에서도 최상위권**에 올랐습니다 ([deepseek-ai/DeepSeek-V2 · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2#:~:text=Alignbench%20%28https%3A%2F%2Farxiv)). 예를 들어 중국어 대화 평가(Alignbench)에서 **GPT-4가 8.01점**일 때, **DeepSeek-V2-Chat (RLHF 버전)은 7.91점**으로 근소한 차이까지 접근했습니다 ([deepseek-ai/DeepSeek-V2 · Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2#:~:text=%E6%A8%A1%E5%9E%8B%20%E5%BC%80%E6%BA%90%2F%E9%97%AD%E6%BA%90%20%E6%80%BB%E5%88%86%20%E4%B8%AD%E6%96%87%E6%8E%A8%E7%90%86%20%E4%B8%AD%E6%96%87%E8%AF%AD%E8%A8%80,17)). 이러한 결과들은 DeepSeek-V2가 **MoE의 효율성을 극대화하면서도 성능을 희생하지 않은 성공적인 사례**임을 나타냅니다.

**요약**: DeepSeek-V2는 기존의 MoE 기반 모델들과 비교해 **MLA를 통한 긴 문맥 처리와 메모리 절감**, **향상된 MoE 게이팅으로 효율적이고 균형잡힌 전문가 활용**, **경제적인 훈련 비용 대비 뛰어난 성능**이라는 **독자적인 강점**을 갖추고 있습니다. 그 성능은 다양한 표준 벤치마크에서 입증되었으며, **적은 활성 파라미터로도 거대 모델 수준의 지능을 구현**했다는 평가를 받고 있습니다 ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=When%20combined%20with%20MLA%2C%20this,a%20drastically%20lower%20compute%20budget)) ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=%2A%20Memory%3A%2093.3,extended%20context%20windows%20by%2032x)). 이러한 기술적 진전 덕분에 DeepSeek-V2는 **학계와 산업계에 큰 반향을 일으켰으며**, 자원 효율적인 거대 언어 모델 개발의 **새로운 가능성**을 제시하였습니다 ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=Except%20DeepSeek%20trounced%20them%20all,3)) ([DeepSeek and the End of an Era - by Ryan Cunningham](https://www.machineyearning.io/p/deepseek-and-the-end-of-an-era?utm_campaign=post&utm_medium=web#:~:text=When%20combined%20with%20MLA%2C%20this,a%20drastically%20lower%20compute%20budget)).

